<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
        <meta name="author" content="Nick Malleson">
        <meta name="keywords" content="Agent-based Modelling; Large language model; Geospatial foundation model; Urban Modelling.">

		<title>Enhancing Spatial Reasoning and Behaviour in Urban ABMs with Large-Language Models and Geospatial Foundation Models</title>

		<link rel="stylesheet" href="reveal/css/reveal.css">
		<link rel="stylesheet" href="reveal/css/theme/white.css">

        <!-- Some extra bits added by Nick -->
        <link rel="stylesheet" href="reveal/css/nick.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="reveal/lib/css/zenburn.css">

		<!-- Pr,inting and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal/css/print/pdf.css' : 'reveal/css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
        
        <!-- For the EnKF formula -->
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script type="text/javascript" async
                src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
        </script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
                
                <section class="title"  id="index" data-background-image="../figures/shutterstock_788458099-small.jpg">
                    <div class="whitebackground2">

                        <h5>CUPUM 2025, UCL, London</h5>

                        <h1 style="font-size: 55px;">Enhancing Spatial Reasoning and Behaviour in Urban ABMs with 
                            Large-Language Models and Geospatial Foundation Models</h1>
                        <hr/>
                        <h2 style="font-size: 35px;">
                            <a href="http://nickmalleson.co.uk/">Nick Malleson<sup></sup></a>, Andrew Crooks, Alison Heppenstall and Ed Manley</h2>
                        <h4>University of Leeds, UK; University at Buffalo, US; University of Glasgow, UK</h4>
                        <h4><a href="mailto:n.s.malleson@leeds.ac.uk">n.s.malleson@leeds.ac.uk</a></h4>
                        <hr/>
                        <p style="text-align:center;font-size:25px">Slides available at: <br/><a href="https://www.nickmalleson.co.uk/presentations.html">www.nickmalleson.co.uk/presentations.html</a></p>

                        <a href="http://www.leeds.ac.uk/">
                            <img style="float:left;padding:10px;width:150px; height:auto;" src="../figures/uol_logo.gif"/>
                        </a>
                        
                        <a href="https://europa.eu/european-union/index_en">
                            <img style="float:center;padding:10px 50px 10px 50px;width:70px; height:auto;" src="../figures/LOGO_EU.jpg"/>
                        </a>

                        <!--<a href="http://www.turing.ac.uk/">
                            <img style="float:right;padding:10px;width:150px; height:auto;" src="../figures/LOGO_TURING.png"/>
                        </a>-->
                    </div>
                </section>
                
                
                
                <section id="context">
                    <h2>Context</h2>
                    <p>Modelling human behaviour in ABMs is (still!) an ongoing challenge</p>
                    <p>Behaviour typically implemented with bespoke rules, but even more advanced mathematical approaches are limitted</p>
                    <p>Can new AI approaches offer a solution?</p>
                    <p class="l2"><strong>Large Language Models</strong> can respond to prompts in 'believable', 'human-like' ways</p>
                    <p class="l2"><strong>Geospatial Foundation Models</strong> capture nuanced, complex associations between spatial objects</p>
                    <p class="l2"><strong>Multimodal Foundation Models</strong> operate with diverse data (text, video, audio, etc.)</p>
                    <p>This talk: discuss the opportunites offered by LLMs, GSMs and MFMs as a means of creating more realistic spatial agents.</p>
                    
                </section>
                
                
                <section id="flexible_behaviour" data-background-image="../figures/the-roaming-platypus-310824-unsplash-small.jpg">
                    <div class="whitebackground">
                        <div class="fragment">
                            <h2>But first, where might this lead...</h2>
                        </div>
                        <div class="fragment">
                            <p>"All models are great, until you need them"</p>
                        </div>
                        <div class="fragment">
                            <p>It's fine to use models under normal conditions. Very useful.</p>
                        </div>
                        <div class="fragment">
                            <p>Especailly if the system undergoes a fundamental change (COVID? Global financial crash?) -- 
                                then we <strong>really</strong> need models to help</p>
                        </div>
                        <div class="fragment">
                            <p>But then they're totally useless!</p>
                        </div>
                    </div>
                </section>
                
                <section data-background-video="../videos/sim.mp4,../videos/sim.webm"
                         data-background-video-loop="loop"
                         id="abm-example-awareness_test"
                          data-transition="fade">
                    <h2>&nbsp;</h2>
                    <div class="fragment whitebackground2" >
                        <h2>Example: a burglary ABM</h2>
                        <h2>&nbsp;</h2>
                        <h2>Worked great, until COVID...</h2>
                    </div>
                    <h2>&nbsp;</h2>
                </section>
                
                <section id="flexible_behaviour3" data-background-image="../figures/the-roaming-platypus-310824-unsplash-small.jpg">
                    <h2>&nbsp;</h2>
                    <div class="whitebackground">
                        <h2>Maybe a model with LLM-backed agents would be 
                            better able to respond after a catastrophic system change</h2>
                    </div>
                </section>
                
                
                
                
                <section id="llms">
                    <h2>Large Language Models (LLMs)</h2>
                    <p>Early evidence suggests that large-language models (LLMs) can be used to represent a wide range 
                        of human behaviours</p>
                    <p>Already a flurry of activity in LLM-backed ABMs</p>
                    <p class="l2">E.g. AutoGPT, BabyAGI, Generative Agents, MetaGP ... and others ... </p>
                    <figure>
                        <img src="../figures/attribution/park_llm_agent.png"
                             style="width:70%;"
                             alt="Image of the ABM created by Park et. al."
                             />
                        <figcaption>Park, Joon Sung, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. 
                            ‘Generative Agents: Interactive Simulacra of Human Behavior’. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, 1–22. San Francisco CA USA: ACM. 
                            DOI: <a href="https://doi.org/10.1145/3586183.3606763">10.1145/3586183.3606763</a>.
                        </figcaption>
                    
                    </figure>
                    
                </section>
                
                <section id="llm_abm_challenges">
                    <h2>LLMs &amp; ABMs: Challenges</h2>
                    <p>Lots of them!</p>
                    <p class="l2">Computational complexity: thousands/millions of LLMs?</p>
                    <p class="l2">Bias: LLMs very unlikely to be representative 
                        (non-English speakers, cultural bias, digital divide, etc.)</p>
                    <p class="l2">Validation: consistency (i.e. stochasticity), robustness (i.e. sensitivity to prompts),
                        halluncinations, train/test contamination, and others</p>
                    <p>Main one for this talk: <strong>the need to interface through text</strong></p>
                    <p class="l2">Maybe reasonning with language makes sense</p>
                    <p class="l2">But having to <i>describe</i> the world with text is a huge simplificaion /
                        abstraction </p>
                </section>
                
                
                <section id="fms">
                    <h2>A solution? Multimodal and Geospatial Foundation Models</h2>
                    <p>Foundation models: "a machine learning or deep learning model trained on vast datasets so that 
                        it can be applied across a wide range of use cases" (Wikipedia)</p>
                    <p class="l2">LLMs are Foundation models that work with text</p>
                    <p>Geospatial Foundation Models</p>
                    <p class="l2">FMs that work with spatial data (street view images, geotagged social media data, 
                        video, GPS trajectories, points-of-interest, etc.) to create rich, multidimensional spatial
                        representations </p>
                    <p>Multimodal Foundation Models</p>
                    <p class="l2">FMs that work with diverse data, e.g. text, audio, image, video, etc.</p>
                </section>
                
                <section id="gfms">
                    <h2>Geospatial Foundation Model Example</h2>
                    <p>XXXX ADD DETAILS -- Huang et al.</p>
                    <figure>
                        <img src="../figures/attribution/huang_urbanclip.png">
                        <figcaption>Balsebre, Pasquale, Weiming Huang, Gao Cong, and Yi Li. 2024. ‘City Foundation Models for Learning General Purpose Representations from OpenStreetMap’. 
                            In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, 87–97. Boise ID USA: ACM..
                            DOI: <a href="https://doi.org/10.1145/3627673.3679662">10.1145/3627673.3679662</a>
                        </figcaption>
                    </figure>
                    
                </section> 
                
                <!-- Weiming land use estimation not quite relevantZ
                <section id="gfms">
                    <h2>Geospatial Foundation Model Example</h2>
                    <p>Huang et al. develop a prompting strategy that allows a vision-language model (CLIP) identify
                    land use in street view data by <i>comparing text and image embeddings</i></p>
                    <figure>
                        <img src="../figures/attribution/huang_urbanclip.jpg">
                        <figcaption>Huang, Weiming, Jing Wang, and Gao Cong. 2024. ‘Zero-Shot Urban Function Inference with Street View Images through Prompting a Pretrained Vision-Language Model’. 
                            <i>International Journal of Geographical Information Science</i> 38 (7): 1414–42. 
                            DOI: <a href="https://doi.org/10.1080/13658816.2024.2347322">10.1080/13658816.2024.2347322</a>.
                        </figcaption>
                    </figure>
                    
                </section> 
                -->

                <section id="towards">
                    <h2>Towards Multi-Modal Foundation Models for ABMs (??)</h2>
                    <p>GFMs and LLMs: a new generation of ABMs?</p>
                    <p class="l2">LLMs 'understand' human behaviour and can reason realistically</p>
                    <p class="l2">GFMs provide nuanced representation of 'space'</p>
                    <div class="fragment">
                        <p>How?</p>
                        <p class="l2"><strike>I've no idea!</strike> Watch this space.</p>
                        <p class="l2">Insert spatial embeddings directly into the LLM?</p>
                        <p class="l2">Use an approach like BLIP-2 that trains a small transformer as an
                        interface between an LLM and a vision-language model</p>
                        <p class="l2">Suggestions welcome!</p>
                    </div>
                </section>
                
                <section id="Challenges" >
                    <h2>Summary and Outlook</h2>
                    <p>Huge potential to use LLMs to drive agents in an ABM</p>
                    <p class="l2">But need to overcome some huge challenges first</p>
                    <p class="l2">Lots of activity, but very little peer-reviewed</p>
                    <p>Geospatial (or multimodal) foundation models could offer a better interface
                    to the environment than text</p>
                    <p>Maybe...</p>
                </section>
                
                <section class="title"  id="thanks" data-background-image="../figures/shutterstock_788458099-small.jpg">
                    <div class="whitebackground2">
                        <h5>CUPUM 2025, UCL, London</h5>
                        <h1 style="font-size: 55px;">Enhancing Spatial Reasoning and Behaviour in Urban ABMs with 
                            Large-Language Models and Geospatial Foundation Models</h1>
                        <hr/>
                        <h2 style="font-size: 35px;">
                            <a href="http://nickmalleson.co.uk/">Nick Malleson<sup></sup></a>, Andrew Crooks, Alison Heppenstall and Ed Manley</h2>
                        <h4>University of Leeds, UK<br/>University at Buffalo, US<br/>University of Glasgow, UK</h4>
                        <h4><a href="mailto:n.s.malleson@leeds.ac.uk">n.s.malleson@leeds.ac.uk</a></h4>
                        <hr/>
                        <p style="text-align:center;font-size:25px">Slides available at: <br/><a href="https://www.nickmalleson.co.uk/presentations.html">www.nickmalleson.co.uk/presentations.html</a></p>

                        <a href="http://www.leeds.ac.uk/">
                            <img style="float:left;padding:10px;width:150px; height:auto;" src="../figures/uol_logo.gif"/>
                        </a>
                        
                        <a href="https://europa.eu/european-union/index_en">
                            <img style="float:center;padding:10px 50px 10px 50px;width:70px; height:auto;" src="../figures/LOGO_EU.jpg"/>
                        </a>

                        <a href="http://www.turing.ac.uk/">
                            <img style="float:right;padding:10px;width:150px; height:auto;" src="../figures/LOGO_TURING.png"/>
                        </a>
                    </div>
                </section>



			</div>
		</div>

		<script src="reveal/lib/js/head.min.js"></script>
		<script src="reveal/js/reveal.js"></script>

		<script>
			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				history: true,

				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: 'reveal/plugin/markdown/marked.js' },
					{ src: 'reveal/plugin/markdown/markdown.js' },
					{ src: 'reveal/plugin/notes/notes.js', async: true },
					{ src: 'reveal/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
