<!doctype html>
<html>

    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
        <meta name="author" content="Nick Malleson">
        <meta name="keywords" content="digital twins, ABM, data assimilation, city simulation, particle filter, uncertainty">

        <title>Foundation Models for Nuanced Agent Behaviours</title>

        <link rel="stylesheet" href="reveal/css/reveal.css">
        <link rel="stylesheet" href="reveal/css/theme/white.css">

        <!-- Some extra bits added by Nick -->
        <link rel="stylesheet" href="reveal/css/nick.css">

        <!-- Theme used for syntax highlighting of code -->
        <link rel="stylesheet" href="reveal/lib/css/zenburn.css">
        
        <!-- For the EnKF formula -->
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script type="text/javascript" async
                src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
        </script>

        <!-- Printing and PDF exports -->
        <script>
            var link = document.createElement('link');
            link.rel = 'stylesheet';
            link.type = 'text/css';
            link.href = window.location.search.match(/print-pdf/gi) ? 'reveal/css/print/pdf.css' : 'reveal/css/print/paper.css';
            document.getElementsByTagName('head')[0].appendChild(link);
        </script>
    </head>

    <body>
        <div class="reveal">
            <div class="slides">

                <section class="title" id="index"
                         data-background-image="../figures/shutterstock_788457058-small.jpg">
                    <div class="whitebackground2">
                        <h5>LLM4ABM SIG Meeting. 8th January 2027.</h5>
                        <hr />
                        <h1 style="font-size: 45px;">Foundation Models for Nuanced Agent Behaviours</h1>
                        <hr />
                        <h2 style="font-size: 35px;">
                            <a href="http://nickmalleson.co.uk/">Nick Malleson</a>, University of Leeds, UK</h2>
                        <h4 style="font-size: 25px;"><a href="mailto:n.s.malleson@leeds.ac.uk">n.s.malleson@leeds.ac.uk</a></h4>
                        <hr/>
                        <p style="text-align:center;font-size:20px">Slides available at: <br />
                            <a href="https://urban-analytics.github.io/dust/presentations.html">https://urban-analytics.github.io/dust/presentations.html</a>
                        </p>
                        <hr />
                        <a href="https://europa.eu/european-union/index_en">
                            <img style="float:left;padding:5px;width:auto; height:60px;"
                                 data-src="../figures/LOGO_EU.jpg" />
                        </a>
                        <a href="http://www.leeds.ac.uk/">
                            <img style="float:right;padding:5px;width:auto; height:60px;"
								 class="whitebackground"
                                 data-src="../figures/uol_logo.gif" />
                        </a>
                        <a href="https://environment.leeds.ac.uk/geography">
                            <img style="padding:10px;width:150px; height:auto;" data-src="../figures/LOGO_SOG.png" />
                        </a>
                        <!-- <a href="http://www.turing.ac.uk/">
                        <img style="float:right;padding:10px;width:150px; height:auto;" src="../figures/LOGO_TURING.png"/>
                        </a> -->
                    </div>
                </section>
                
                <section id="outline">
                    <img class="right"
                         src="../figures/chatgpt/digital_brain.png"
                         alt="ChatGPT-generated image of a digital brain" />
                    <h2>Talk Outline</h2>
                    <p>Some ideas about whether foundation models (and/or large-language models) could be helpful
                    for modelling agent behaviour in ABMs</p>
                    <p>Some ideas about what we need to do to allow them to be useful</p>
                    <p>Please be critical!</p>
                </section>
                
                
                <!-- ************************************************************
                ************************* Context  ************************
                ************************************************************ --> 
                
                <section id="vision">
                    
                    <h2>Context</h2>
                    <p>Agent behaviours are based on historical precedents ... </p>
                    <p class="l2">Behavioural theories</p>
                    <p class="l2">Empirical evidence</p>
                    <p>Commonly implemented using:</p>
                    <p class="l2">pre-defined rules</p>
                    <p class="l2">deliberative frameworks</p>
                    <p class="l2">black box statistical models</p>
                    <p>These work well when the system is <i>mostly</i> in equilibrium</p>                
                </section>
                
                <section id="vision2">
                    <h2>Context</h2>
                    <img src="../figures/explosion.png" class="right" alt="An explosion" />
                    <p><strong>BUT: </strong> what happens if there is a catastrophic, systemic change?</p>
                    <p class="l2">2008 global financial crisis</p>
                    <p class="l2">COVID</p>
                    <p><i>Models based on historical behavioural assumptions can break down</i></p>
                    <p>We (researchers) cannot hope to predict these events, nor how people will behave afterwards</p>
                </section>
                
                <section id="burglary_example">
                    <h2>Example: burglary simulation</h2>
                    <video controls autoplay width="640">
                        <source src="../videos/sim.mp4" type="video/mp4">
                        <source src="../videos/sim.webm" type="video/webm">
                        Your browser does not support the video tag.
                    </video>
                    <p>What happens if the agents can't leave the house?</p>
                </section>

                <section id="llms">
                    <h2>Large Language Models (LLMs)</h2>
                    <p>Early evidence suggests that large-language models (LLMs) can be used to represent a wide range
                        of human behaviours</p>
                    <figure class="right">
                        <img data-src="../figures/attribution/park_llm_agent.png"
                             style="width:100%;"
                             alt="Image of the ABM created by Park et. al."
                             />
                        <figcaption>Park e al. (2023)
                            ‘Generative Agents: Interactive Simulacra of Human Behavior’. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, 1–22. San Francisco CA USA: ACM.
                            DOI: <a href="https://doi.org/10.1145/3586183.3606763">10.1145/3586183.3606763</a>.
                        </figcaption>
                    </figure>
                    <p>Already a flurry of activity in LLM-backed ABMs</p>
                    <p class="l2">E.g. METAAGENTS, AgentSociety, Shachi, Concordia, MetaGPT ... and others ... </p>
                    <p>But efforts are <i>emerging prototypes</i>.</p>
                    <p>Limited peer review</p>
                </section>

                <section id="geofm">
                    <figure class="right">
                        <img src="../figures/attribution/huang_urbanclip.png"
                             alt="A diagram showing how UrbanCLIP creates place embeddings from OpenStreetMap data"
                        />
                        <figcaption>UrbanClip: Balsebre et al. (2024) ‘City Foundation Models for Learning General Purpose Representations from OpenStreetMap’.
                            In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, 87–97. Boise ID USA: ACM..
                            DOI: <a href="https://doi.org/10.1145/3627673.3679662">10.1145/3627673.3679662</a></figcaption>

                        <p></p> <!-- make a bit of space between images-->

                        <img data-src="../figures/alpha_earth.jpg"
                             alt="A detailed collage of 20 multi-colored tiles featuring high-resolution satellite imagery of various landscapes, illustrating global mapping data"
                        />
                        <figcaption>Google <a src="http://www.deepmind.com/research/highlighted-research/alphaearth">
                            AlphaEarth</a> have released global embeddings from satellite images.</figcaption>
                    </figure>
                    <h2>Geospatial Foundation Models</h2>
                    <p>Foundation models trained on spatial data (sometimes including text)</p>
                    <p class="l2">E.g. UrbanCLIP, CityFM, GeoGPT, etc.</p>
                    <p>'Transformers for spatial data'.</p>
                    <p>Place embeddings capture <i>spatial structure and scale</i> and can be used to
                        perform advanced spatial reasoning.</p>

                </section>
                <!--<section id="gfms">
                    <h2>Geospatial Foundation Model Example</h2>
                    <p>Example of a foundation model constructed using OSM data</p>
                    <p class="l2">Embeddings are remarkably good at predicting things like traffic speed and
                        building functionality (zero-shot)</p>
                    <figure>
                        <img src="../figures/attribution/huang_urbanclip.png"
						style="width:70%;">
                        <figcaption>Balsebre, Pasquale, Weiming Huang, Gao Cong, and Yi Li. 2024. ‘City Foundation Models for Learning General Purpose Representations from OpenStreetMap’.
                            In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, 87–97. Boise ID USA: ACM..
                            DOI: <a href="https://doi.org/10.1145/3627673.3679662">10.1145/3627673.3679662</a>
                        </figcaption>
                    </figure>
                </section>  -->

                <!-- Weiming land use estimation not quite relevantZ
                <section id="gfms">
                    <h2>Geospatial Foundation Model Example</h2>
                    <p>Huang et al. develop a prompting strategy that allows a vision-language model (CLIP) identify
                    land use in street view data by <i>comparing text and image embeddings</i></p>
                    <figure>
                        <img src="../figures/attribution/huang_urbanclip.jpg">
                        <figcaption>Huang, Weiming, Jing Wang, and Gao Cong. 2024. ‘Zero-Shot Urban Function Inference with Street View Images through Prompting a Pretrained Vision-Language Model’.
                            <i>International Journal of Geographical Information Science</i> 38 (7): 1414–42.
                            DOI: <a href="https://doi.org/10.1080/13658816.2024.2347322">10.1080/13658816.2024.2347322</a>.
                        </figcaption>
                    </figure>

                </section>
                -->

                <!-- ************************************************************
                ************************* Vision  ************************
                ************************************************************ -->

                <section id="vision3">
                    <h2>Vision</h2>
                    <h3>Agents backed by foundation models</h3>
                    <img src="../figures/llm_agents.png"
                         alt="Diagram showing traditional ABM v.s. one where the agents are controlled by LLMs"/>
                </section>
                
                <section id="fm-llms">
                    <h2>Vision</h2>
                    <h3>Agents backed by foundation models</h3>
                    <p><strong>Large Language Models</strong> can respond to prompts in 'believable', 'human-like' ways</p>
                    <p><strong>Geospatial Foundation Models</strong> capture nuanced, complex spatial associations </p>
                    <!-- <p><strong>Multi-modal Foundation Models</strong> operate with diverse data (text, video, audio, etc.)</p> -->
                    <p>Together, these models could allow us to create ABMs where agents behave in realistic ways
                        even when faced with unprecedented situations</p>
                </section>

                <!--
                <section id="vision4">
                    <h2>Vision</h2>
                    <h3>Agents backed by foundation models</h3>
                    <p>XXXX killer app.. - talk about empirical applications here. Outline what is possible.</p>
                </section>
                -->
                
                <!-- ************************************************************
                    ************************* Challenges  ************************
                    ************************************************************ -->

                <section id="challenges">
                    <h2>(some) Challenges</h2>
                    <p>Environmental adaptability</p>
                    <p class="l2">Agent reasoning might be unbounded, but their actions are limited by the simulation
                    environment</p>
                    <p>Environment perception</p>
                    <p class="l2">LLMs can only operate on text. Leads to highly simplified contextual descriptions.</p>
                    <p>Operational challenges</p>
                    <p class="l2">Computational complexity, validation (data leakage), calibration</p>
                </section>

                <section id="environmental_adaptability">
                    <h2>Challenge 1: Environment adaptability</h2>
                    <p>LLM reasoning is 'unbounded'</p>
                    <p class="l2">Generative agents can articulate any linguistically desirable response</p>
                    <p>But, in practice, their actions are currently limited to a small number of cases that model
                        developers have presupposed.</p>
                    <p class="l2"><i>What if my burglar agents wanted to target commercial buildings?</i></p>
                    <p class="l2"><i>What if an agent is lonely during lockdown and wants to get a dog?</i></p>
                    <p>Potential solution: <strong>adaptive simulation environments</strong></p>
                </section>

                <section id="environmental_adaptability2">
                    <h2>Dynamic simulation environments</h2>
                    <p>Surprisingly little research on this!</p>
                    <p>Propose LLM-backed 'ask-detect-extend' cycle:</p>
                    <p class="l2">Ask: agent requests an environmental feature</p>
                    <p class="l2">Detect: System detects whether the feature lies outside current capabilities</p>
                    <p class="l2">Extend: If necessary, generate new functionality</p>
                    <p>Can include researcher in-the-loop for non-trivial extensions</p>
                    <p>Implementation: move from text-based world to more complex spatial environments</p>
                </section>

                <section id="text_prompting">
                    <figure class="right" style="width: 40%">
                        <img data-src="../figures/view_of_a_street.png"
                             alt="An image of a street with buildings and trees"
                        />
                        <figcaption>
                            How should this environment be described to an LLM agent using text?
                        </figcaption>
                    </figure>
                    <h2>Challenge 2: Environment perception</h2>
                    <p>Humans integrate numerous sensory cues to conceptualise their surroundings</p>
                    <p>But generative agents must have their environments described to them in text</p>
                    <p>This removes the richness and nuance from the agent's local context</p>
                    <p>Potential solution: <strong>multi-modal agents</strong></p>
                </section>

                <section id="text_prompting2">
                    <h2>Multi-modal agents</h2>
                    <p>Couple LLMs (for reasoning) with GeoFMs (for spatial perception).</p>
                    <p>Potential solutions:</p>
                    <p class="l2"><i>Gated cross-attention</i> (e.g. Flamingo)</p>
                    <p class="l2"><i>Text/image token interleaving</i> (e.g. PaLM-E)</p>
                    <p class="l2"><i>A separate, mini transformer</i> (e.g. BLIP-2's Q-Former)</p>
                    <p>Training:</p>
                    <p class="l2">Freeze the main LLM &amp; GeoFM weights and train a subset (computationally feasible)</p>
                    <p class="l2">Will need data though. Generate synthetically?</p>
                </section>

                <section id="enabling_methods">
                    <h2>Challenge 3: Operational barriers</h2>
                    <p>Computational complexity: thousands/millions of LLMs?</p>
                    <p class="l2">Solutions: cache queries, emulation, prioritise smaller models where possible, ... </p>
                    <p class="l2"><i>This is a problem that is being actively researched by others</i></p>
                    <p>Calibration with no tunable parameters</p>
                    <p class="l2">Solutions: LLM fine-tuning with domain-specific data; proxy parameters that
                        affect prompts; calibrating the <i>environment</i>.</p>
                    <p>Preventing data leakage in validation</p>
                    <p class="l2">LLMs have 'seen' most major historical events, so how can we validate
                        the new approach?</p>
                    <p class="l2">Solutions: use early LLMs and pre-COVID data (e.g. 'The Pile') to test
                        whether <i>principle</i> of adapting reasoning can emerge</p>
                </section>



                <!-- ************************************************************
                ************** CONCLUSION / ACKNOWLEDGEMENTS ********************
                ************************************************************ -->
                
               <section id="conclusions">
                    <h2>Conclusions</h2>
                   <p>ABMs will struggle to represent agent behaviours in unprecedented situations</p>
                   <p>Foundation models (LLMs, GeoFMs) offer a potential solution;
                       allowing agents to behave in reasonable ways in novel situations</p>
                   <p>Significant technical and methodological challenges remain:</p>
                   <p class="l2">Environmental adaptability</p>
                   <p class="l2">Environment perception</p>
                   <p class="l2">Operational challenges</p>
               </section>



                <section class="title" id="thanks"
                         data-background-image="../figures/shutterstock_788457058-small.jpg">
                    <div class="whitebackground2">
                        <h5>LLM4ABM SIG Meeting. 8th January 2027.</h5>
                        <hr />
                        <h1 style="font-size: 45px;">Foundation Models for Nuanced Agent Behaviours</h1>
                        <hr />
                        <h2 style="font-size: 35px;">
                            <a href="http://nickmalleson.co.uk/">Nick Malleson</a>, University of Leeds, UK</h2>
                        <h4 style="font-size: 25px;"><a href="mailto:n.s.malleson@leeds.ac.uk">n.s.malleson@leeds.ac.uk</a></h4>
                        <hr/>
                        <p style="text-align:center;font-size:20px">Slides available at: <br />
                            <a href="https://urban-analytics.github.io/dust/presentations.html">https://urban-analytics.github.io/dust/presentations.html</a>
                        </p>
                        <hr />
                        <a href="https://europa.eu/european-union/index_en">
                            <img style="float:left;padding:5px;width:auto; height:60px;"
                                 data-src="../figures/LOGO_EU.jpg" />
                        </a>
                        <a href="http://www.leeds.ac.uk/">
                            <img style="float:right;padding:5px;width:auto; height:60px;"
								 class="whitebackground"
                                 data-src="../figures/uol_logo.gif" />
                        </a>
                        <a href="https://environment.leeds.ac.uk/geography">
                            <img style="padding:10px;width:150px; height:auto;" data-src="../figures/LOGO_SOG.png" />
                        </a>
                        <!-- <a href="http://www.turing.ac.uk/">
                        <img style="float:right;padding:10px;width:150px; height:auto;" src="../figures/LOGO_TURING.png"/>
                        </a> -->
                    </div>
                </section>

            </div>
        </div>

        <script src="reveal/lib/js/head.min.js"></script>
        <script src="reveal/js/reveal.js"></script>

        <script>
            // More info https://github.com/hakimel/reveal.js#configuration
            Reveal.initialize({
                history: true,

                // More info https://github.com/hakimel/reveal.js#dependencies
                dependencies: [
                    { src: 'reveal/plugin/markdown/marked.js' },
                    { src: 'reveal/plugin/markdown/markdown.js' },
                    { src: 'reveal/plugin/notes/notes.js', async: true },
                    { src: 'reveal/plugin/highlight/highlight.js', async: true, callback: function () { hljs.initHighlightingOnLoad(); } }
                ]
            });
        </script>
    </body>

</html>
